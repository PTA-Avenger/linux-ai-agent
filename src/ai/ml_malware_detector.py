"""
Machine Learning-based Malware Detector for Linux AI Agent.
Uses feature extraction and ML models to detect malware patterns.
"""

import os
import sys
import json
import hashlib
import numpy as np
import pandas as pd
from typing import Dict, List, Optional, Any, Tuple
from pathlib import Path
import pickle
import time
import math

sys.path.append(os.path.join(os.path.dirname(__file__), '../utils'))
from utils import get_logger, log_operation

# Import ML libraries
try:
    from sklearn.ensemble import RandomForestClassifier, IsolationForest
    from sklearn.linear_model import LogisticRegression
    from sklearn.svm import SVC
    from sklearn.preprocessing import StandardScaler, LabelEncoder
    from sklearn.model_selection import train_test_split, cross_val_score
    from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score
    from sklearn.feature_extraction.text import TfidfVectorizer
    import joblib
    SKLEARN_AVAILABLE = True
except ImportError:
    print("scikit-learn not available for ML malware detection")
    SKLEARN_AVAILABLE = False

try:
    import tensorflow as tf
    from tensorflow import keras
    from tensorflow.keras import layers
    TENSORFLOW_AVAILABLE = True
except ImportError:
    TENSORFLOW_AVAILABLE = False

logger = get_logger("ml_malware_detector")


class FeatureExtractor:
    """
    Extract features from files for malware detection.
    """
    
    def __init__(self):
        self.pe_features = [
            'file_size', 'entropy', 'sections_count', 'imports_count',
            'exports_count', 'has_debug_info', 'is_packed', 'compilation_timestamp'
        ]
        
        self.string_features = [
            'suspicious_strings_count', 'crypto_strings_count', 'network_strings_count',
            'system_calls_count', 'registry_strings_count'
        ]
        
        # Suspicious patterns
        self.suspicious_patterns = [
            b'CreateRemoteThread', b'VirtualAllocEx', b'WriteProcessMemory',
            b'SetWindowsHookEx', b'GetProcAddress', b'LoadLibrary',
            b'ShellExecute', b'WinExec', b'CreateProcess', b'RegOpenKeyEx',
            b'RegSetValueEx', b'InternetOpen', b'HttpSendRequest',
            b'socket', b'connect', b'send', b'recv'
        ]
        
        self.crypto_patterns = [
            b'CryptAcquireContext', b'CryptCreateHash', b'CryptEncrypt',
            b'CryptDecrypt', b'MD5', b'SHA1', b'SHA256', b'AES', b'DES', b'RSA'
        ]
        
        self.network_patterns = [
            b'WSAStartup', b'gethostbyname', b'inet_addr', b'htons',
            b'http://', b'https://', b'ftp://', b'tcp://', b'udp://'
        ]
    
    def extract_features(self, file_path: str) -> Dict[str, Any]:
        """
        Extract comprehensive features from a file.
        
        Args:
            file_path: Path to the file to analyze
            
        Returns:
            Dictionary of extracted features
        """
        features = {}
        
        try:
            file_path = Path(file_path)
            
            if not file_path.exists():
                return {"error": "File not found"}
            
            # Basic file features
            features.update(self._extract_basic_features(file_path))
            
            # Content-based features
            features.update(self._extract_content_features(file_path))
            
            # String-based features
            features.update(self._extract_string_features(file_path))
            
            # Statistical features
            features.update(self._extract_statistical_features(file_path))
            
            # Metadata features
            features.update(self._extract_metadata_features(file_path))
            
            log_operation(logger, "EXTRACT_FEATURES", {
                "file_path": str(file_path),
                "features_count": len(features),
                "file_size": features.get("file_size", 0)
            })
            
        except Exception as e:
            logger.error(f"Error extracting features from {file_path}: {e}")
            features = {"error": str(e)}
        
        return features
    
    def _extract_basic_features(self, file_path: Path) -> Dict[str, Any]:
        """Extract basic file features."""
        features = {}
        
        try:
            stat = file_path.stat()
            
            features['file_size'] = stat.st_size
            features['creation_time'] = stat.st_ctime
            features['modification_time'] = stat.st_mtime
            features['access_time'] = stat.st_atime
            
            # File extension
            features['file_extension'] = file_path.suffix.lower()
            features['has_extension'] = bool(file_path.suffix)
            
            # File name characteristics
            name = file_path.name.lower()
            features['name_length'] = len(name)
            features['name_has_numbers'] = any(c.isdigit() for c in name)
            features['name_has_special_chars'] = any(c in '!@#$%^&*()_+-=[]{}|;:,.<>?' for c in name)
            
            # Suspicious file names
            suspicious_names = ['temp', 'tmp', 'cache', 'system', 'windows', 'microsoft']
            features['name_is_suspicious'] = any(sus in name for sus in suspicious_names)
            
        except Exception as e:
            logger.warning(f"Error extracting basic features: {e}")
        
        return features
    
    def _extract_content_features(self, file_path: Path) -> Dict[str, Any]:
        """Extract content-based features."""
        features = {}
        
        try:
            with open(file_path, 'rb') as f:
                content = f.read(min(1024 * 1024, file_path.stat().st_size))  # Read max 1MB
            
            if not content:
                return features
            
            # Entropy calculation
            features['entropy'] = self._calculate_entropy(content)
            
            # Byte frequency analysis
            byte_counts = np.bincount(np.frombuffer(content, dtype=np.uint8), minlength=256)
            features['byte_entropy'] = self._calculate_entropy_from_counts(byte_counts)
            
            # Most common bytes
            most_common_byte = np.argmax(byte_counts)
            features['most_common_byte'] = most_common_byte
            features['most_common_byte_freq'] = byte_counts[most_common_byte] / len(content)
            
            # Null byte analysis
            features['null_byte_count'] = byte_counts[0]
            features['null_byte_ratio'] = byte_counts[0] / len(content)
            
            # Printable character ratio
            printable_count = sum(1 for b in content if 32 <= b <= 126)
            features['printable_ratio'] = printable_count / len(content)
            
            # High entropy regions (potential encryption/packing)
            features['high_entropy_regions'] = self._count_high_entropy_regions(content)
            
            # Magic bytes detection
            features.update(self._detect_magic_bytes(content))
            
        except Exception as e:
            logger.warning(f"Error extracting content features: {e}")
        
        return features
    
    def _extract_string_features(self, file_path: Path) -> Dict[str, Any]:
        """Extract string-based features."""
        features = {}
        
        try:
            with open(file_path, 'rb') as f:
                content = f.read(min(1024 * 1024, file_path.stat().st_size))  # Read max 1MB
            
            # Extract strings
            strings = self._extract_strings(content)
            
            features['total_strings'] = len(strings)
            features['avg_string_length'] = np.mean([len(s) for s in strings]) if strings else 0
            features['max_string_length'] = max([len(s) for s in strings]) if strings else 0
            
            # Count suspicious patterns
            features['suspicious_strings_count'] = self._count_patterns(content, self.suspicious_patterns)
            features['crypto_strings_count'] = self._count_patterns(content, self.crypto_patterns)
            features['network_strings_count'] = self._count_patterns(content, self.network_patterns)
            
            # URL and IP detection
            features['url_count'] = self._count_urls(strings)
            features['ip_count'] = self._count_ips(strings)
            
            # Registry keys (Windows-specific)
            registry_patterns = [b'HKEY_', b'SOFTWARE\\', b'SYSTEM\\']
            features['registry_strings_count'] = self._count_patterns(content, registry_patterns)
            
        except Exception as e:
            logger.warning(f"Error extracting string features: {e}")
        
        return features
    
    def _extract_statistical_features(self, file_path: Path) -> Dict[str, Any]:
        """Extract statistical features."""
        features = {}
        
        try:
            with open(file_path, 'rb') as f:
                content = f.read(min(1024 * 1024, file_path.stat().st_size))  # Read max 1MB
            
            if not content:
                return features
            
            # Convert to numpy array for statistical analysis
            data = np.frombuffer(content, dtype=np.uint8)
            
            features['mean_byte_value'] = np.mean(data)
            features['std_byte_value'] = np.std(data)
            features['min_byte_value'] = np.min(data)
            features['max_byte_value'] = np.max(data)
            features['median_byte_value'] = np.median(data)
            
            # Skewness and kurtosis (simplified)
            features['byte_range'] = np.max(data) - np.min(data)
            
            # Byte transition analysis
            transitions = np.diff(data.astype(np.int16))
            features['avg_byte_transition'] = np.mean(np.abs(transitions))
            features['max_byte_transition'] = np.max(np.abs(transitions))
            
            # Longest repeating sequence
            features['longest_repeating_sequence'] = self._find_longest_repeating_sequence(data)
            
        except Exception as e:
            logger.warning(f"Error extracting statistical features: {e}")
        
        return features
    
    def _extract_metadata_features(self, file_path: Path) -> Dict[str, Any]:
        """Extract metadata features."""
        features = {}
        
        try:
            # File hash
            features['md5_hash'] = self._calculate_file_hash(file_path, 'md5')
            features['sha1_hash'] = self._calculate_file_hash(file_path, 'sha1')
            features['sha256_hash'] = self._calculate_file_hash(file_path, 'sha256')
            
            # File age
            current_time = time.time()
            stat = file_path.stat()
            features['file_age_days'] = (current_time - stat.st_mtime) / 86400
            
            # Path analysis
            path_str = str(file_path).lower()
            suspicious_paths = ['temp', 'tmp', 'cache', 'appdata', 'programdata']
            features['in_suspicious_path'] = any(sus_path in path_str for sus_path in suspicious_paths)
            
            features['path_depth'] = len(file_path.parts)
            
        except Exception as e:
            logger.warning(f"Error extracting metadata features: {e}")
        
        return features
    
    def _calculate_entropy(self, data: bytes) -> float:
        """Calculate Shannon entropy of data."""
        if not data:
            return 0.0
        
        # Count byte frequencies
        byte_counts = np.bincount(np.frombuffer(data, dtype=np.uint8), minlength=256)
        probabilities = byte_counts / len(data)
        
        # Calculate entropy
        entropy = 0.0
        for p in probabilities:
            if p > 0:
                entropy -= p * math.log2(p)
        
        return entropy
    
    def _calculate_entropy_from_counts(self, counts: np.ndarray) -> float:
        """Calculate entropy from byte counts."""
        total = np.sum(counts)
        if total == 0:
            return 0.0
        
        probabilities = counts / total
        entropy = 0.0
        for p in probabilities:
            if p > 0:
                entropy -= p * math.log2(p)
        
        return entropy
    
    def _count_high_entropy_regions(self, data: bytes, window_size: int = 256, threshold: float = 7.0) -> int:
        """Count regions with high entropy (potential encryption/packing)."""
        if len(data) < window_size:
            return 0
        
        high_entropy_count = 0
        for i in range(0, len(data) - window_size, window_size):
            window = data[i:i + window_size]
            entropy = self._calculate_entropy(window)
            if entropy > threshold:
                high_entropy_count += 1
        
        return high_entropy_count
    
    def _detect_magic_bytes(self, data: bytes) -> Dict[str, bool]:
        """Detect file type based on magic bytes."""
        magic_signatures = {
            'is_pe': data.startswith(b'MZ'),
            'is_elf': data.startswith(b'\x7fELF'),
            'is_zip': data.startswith(b'PK'),
            'is_pdf': data.startswith(b'%PDF'),
            'is_jpeg': data.startswith(b'\xff\xd8\xff'),
            'is_png': data.startswith(b'\x89PNG'),
            'is_gif': data.startswith(b'GIF8'),
        }
        
        return magic_signatures
    
    def _extract_strings(self, data: bytes, min_length: int = 4) -> List[str]:
        """Extract printable strings from binary data."""
        strings = []
        current_string = ""
        
        for byte in data:
            if 32 <= byte <= 126:  # Printable ASCII
                current_string += chr(byte)
            else:
                if len(current_string) >= min_length:
                    strings.append(current_string)
                current_string = ""
        
        # Don't forget the last string
        if len(current_string) >= min_length:
            strings.append(current_string)
        
        return strings
    
    def _count_patterns(self, data: bytes, patterns: List[bytes]) -> int:
        """Count occurrences of patterns in data."""
        count = 0
        data_lower = data.lower()
        
        for pattern in patterns:
            count += data_lower.count(pattern.lower())
        
        return count
    
    def _count_urls(self, strings: List[str]) -> int:
        """Count URLs in strings."""
        import re
        url_pattern = re.compile(r'https?://[^\s]+', re.IGNORECASE)
        
        count = 0
        for string in strings:
            count += len(url_pattern.findall(string))
        
        return count
    
    def _count_ips(self, strings: List[str]) -> int:
        """Count IP addresses in strings."""
        import re
        ip_pattern = re.compile(r'\b(?:[0-9]{1,3}\.){3}[0-9]{1,3}\b')
        
        count = 0
        for string in strings:
            count += len(ip_pattern.findall(string))
        
        return count
    
    def _find_longest_repeating_sequence(self, data: np.ndarray, max_check: int = 1000) -> int:
        """Find longest repeating byte sequence."""
        if len(data) < 2:
            return 0
        
        # Check only first max_check bytes for performance
        check_data = data[:max_check]
        max_length = 0
        
        for i in range(len(check_data) - 1):
            length = 1
            while (i + length < len(check_data) and 
                   check_data[i] == check_data[i + length]):
                length += 1
            max_length = max(max_length, length)
        
        return max_length
    
    def _calculate_file_hash(self, file_path: Path, algorithm: str) -> str:
        """Calculate file hash."""
        hash_func = getattr(hashlib, algorithm)()
        
        try:
            with open(file_path, 'rb') as f:
                for chunk in iter(lambda: f.read(4096), b""):
                    hash_func.update(chunk)
            return hash_func.hexdigest()
        except Exception:
            return ""


class MLMalwareDetector:
    """
    Machine Learning-based malware detector.
    """
    
    def __init__(self, model_path: str = "ml_models"):
        self.model_path = Path(model_path)
        self.model_path.mkdir(exist_ok=True)
        
        self.feature_extractor = FeatureExtractor()
        
        # ML Models
        self.models = {}
        self.scalers = {}
        self.feature_names = []
        
        # Model configurations
        self.model_configs = {
            'random_forest': {
                'class': RandomForestClassifier,
                'params': {'n_estimators': 100, 'random_state': 42, 'n_jobs': -1}
            },
            'logistic_regression': {
                'class': LogisticRegression,
                'params': {'random_state': 42, 'max_iter': 1000}
            },
            'svm': {
                'class': SVC,
                'params': {'kernel': 'rbf', 'probability': True, 'random_state': 42}
            }
        }
        
        # Anomaly detection model
        self.anomaly_detector = None
        
        # Training data
        self.training_data = []
        self.training_labels = []
        
        # Load existing models
        self._load_models()
        
        logger.info("ML Malware Detector initialized")
    
    def analyze_file(self, file_path: str) -> Dict[str, Any]:
        """
        Analyze a file for malware using ML models.
        
        Args:
            file_path: Path to the file to analyze
            
        Returns:
            Analysis results with predictions from multiple models
        """
        try:
            # Extract features
            features = self.feature_extractor.extract_features(file_path)
            
            if 'error' in features:
                return {
                    'error': features['error'],
                    'file_path': file_path
                }
            
            # Convert features to vector
            feature_vector = self._features_to_vector(features)
            
            if feature_vector is None:
                return {
                    'error': 'Failed to convert features to vector',
                    'file_path': file_path
                }
            
            results = {
                'file_path': file_path,
                'features': features,
                'predictions': {},
                'ensemble_prediction': None,
                'confidence': 0.0,
                'anomaly_score': None
            }
            
            # Make predictions with each model
            predictions = []
            confidences = []
            
            for model_name, model in self.models.items():
                if model is not None and model_name in self.scalers:
                    try:
                        # Scale features
                        scaled_features = self.scalers[model_name].transform([feature_vector])
                        
                        # Predict
                        prediction = model.predict(scaled_features)[0]
                        confidence = model.predict_proba(scaled_features)[0].max()
                        
                        results['predictions'][model_name] = {
                            'prediction': int(prediction),
                            'confidence': float(confidence),
                            'is_malware': bool(prediction)
                        }
                        
                        predictions.append(prediction)
                        confidences.append(confidence)
                        
                    except Exception as e:
                        logger.warning(f"Error with model {model_name}: {e}")
            
            # Ensemble prediction (majority vote)
            if predictions:
                ensemble_pred = 1 if sum(predictions) > len(predictions) / 2 else 0
                avg_confidence = np.mean(confidences)
                
                results['ensemble_prediction'] = {
                    'prediction': int(ensemble_pred),
                    'confidence': float(avg_confidence),
                    'is_malware': bool(ensemble_pred)
                }
            
            # Anomaly detection
            if self.anomaly_detector is not None:
                try:
                    anomaly_score = self.anomaly_detector.decision_function([feature_vector])[0]
                    is_anomaly = self.anomaly_detector.predict([feature_vector])[0] == -1
                    
                    results['anomaly_detection'] = {
                        'score': float(anomaly_score),
                        'is_anomaly': bool(is_anomaly)
                    }
                    
                except Exception as e:
                    logger.warning(f"Error in anomaly detection: {e}")
            
            log_operation(logger, "ANALYZE_FILE", {
                "file_path": file_path,
                "models_used": len(predictions),
                "ensemble_prediction": results.get('ensemble_prediction', {}).get('is_malware', None)
            })
            
            return results
            
        except Exception as e:
            logger.error(f"Error analyzing file {file_path}: {e}")
            return {
                'error': str(e),
                'file_path': file_path
            }
    
    def train_models(self, training_data_path: Optional[str] = None, 
                    synthetic_data_count: int = 1000) -> Dict[str, Any]:
        """
        Train ML models for malware detection.
        
        Args:
            training_data_path: Path to training data (CSV format)
            synthetic_data_count: Number of synthetic samples to generate if no training data
            
        Returns:
            Training results and metrics
        """
        if not SKLEARN_AVAILABLE:
            return {'error': 'scikit-learn not available for training'}
        
        try:
            # Load or generate training data
            if training_data_path and Path(training_data_path).exists():
                data, labels = self._load_training_data(training_data_path)
            else:
                logger.info("No training data provided, generating synthetic data")
                data, labels = self._generate_synthetic_data(synthetic_data_count)
            
            if len(data) == 0:
                return {'error': 'No training data available'}
            
            # Prepare feature matrix
            feature_matrix = np.array(data)
            
            # Store feature names for consistency
            if not self.feature_names:
                self.feature_names = [f'feature_{i}' for i in range(feature_matrix.shape[1])]
            
            # Split data
            X_train, X_test, y_train, y_test = train_test_split(
                feature_matrix, labels, test_size=0.2, random_state=42, stratify=labels
            )
            
            training_results = {}
            
            # Train each model
            for model_name, config in self.model_configs.items():
                try:
                    logger.info(f"Training {model_name}...")
                    
                    # Create and train scaler
                    scaler = StandardScaler()
                    X_train_scaled = scaler.fit_transform(X_train)
                    X_test_scaled = scaler.transform(X_test)
                    
                    # Create and train model
                    model = config['class'](**config['params'])
                    model.fit(X_train_scaled, y_train)
                    
                    # Evaluate model
                    train_score = model.score(X_train_scaled, y_train)
                    test_score = model.score(X_test_scaled, y_test)
                    
                    # Cross-validation
                    cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5)
                    
                    # Store model and scaler
                    self.models[model_name] = model
                    self.scalers[model_name] = scaler
                    
                    training_results[model_name] = {
                        'train_accuracy': float(train_score),
                        'test_accuracy': float(test_score),
                        'cv_mean': float(cv_scores.mean()),
                        'cv_std': float(cv_scores.std())
                    }
                    
                    logger.info(f"{model_name} - Test Accuracy: {test_score:.3f}")
                    
                except Exception as e:
                    logger.error(f"Error training {model_name}: {e}")
                    training_results[model_name] = {'error': str(e)}
            
            # Train anomaly detector
            try:
                logger.info("Training anomaly detector...")
                self.anomaly_detector = IsolationForest(
                    contamination=0.1, random_state=42, n_jobs=-1
                )
                
                # Use only normal samples for anomaly detection training
                normal_indices = np.where(np.array(labels) == 0)[0]
                if len(normal_indices) > 0:
                    X_normal = feature_matrix[normal_indices]
                    scaler_anomaly = StandardScaler()
                    X_normal_scaled = scaler_anomaly.fit_transform(X_normal)
                    
                    self.anomaly_detector.fit(X_normal_scaled)
                    self.scalers['anomaly'] = scaler_anomaly
                    
                    training_results['anomaly_detector'] = {'status': 'trained'}
                
            except Exception as e:
                logger.error(f"Error training anomaly detector: {e}")
                training_results['anomaly_detector'] = {'error': str(e)}
            
            # Save models
            self._save_models()
            
            training_results['summary'] = {
                'total_samples': len(data),
                'malware_samples': sum(labels),
                'benign_samples': len(labels) - sum(labels),
                'feature_count': feature_matrix.shape[1],
                'models_trained': len([r for r in training_results.values() 
                                    if isinstance(r, dict) and 'test_accuracy' in r])
            }
            
            logger.info("Model training completed")
            return training_results
            
        except Exception as e:
            logger.error(f"Error in model training: {e}")
            return {'error': str(e)}
    
    def _features_to_vector(self, features: Dict[str, Any]) -> Optional[np.ndarray]:
        """Convert feature dictionary to vector."""
        try:
            # Define expected features in order
            expected_features = [
                'file_size', 'entropy', 'null_byte_ratio', 'printable_ratio',
                'suspicious_strings_count', 'crypto_strings_count', 'network_strings_count',
                'mean_byte_value', 'std_byte_value', 'high_entropy_regions',
                'longest_repeating_sequence', 'name_length', 'file_age_days',
                'path_depth', 'url_count', 'ip_count'
            ]
            
            # Convert boolean features to int
            vector = []
            for feature_name in expected_features:
                value = features.get(feature_name, 0)
                
                # Handle different data types
                if isinstance(value, bool):
                    vector.append(int(value))
                elif isinstance(value, (int, float)):
                    vector.append(float(value))
                else:
                    vector.append(0.0)
            
            return np.array(vector)
            
        except Exception as e:
            logger.error(f"Error converting features to vector: {e}")
            return None
    
    def _load_training_data(self, data_path: str) -> Tuple[List[List[float]], List[int]]:
        """Load training data from CSV file."""
        try:
            df = pd.read_csv(data_path)
            
            # Assume last column is label
            labels = df.iloc[:, -1].tolist()
            features = df.iloc[:, :-1].values.tolist()
            
            logger.info(f"Loaded {len(features)} training samples from {data_path}")
            return features, labels
            
        except Exception as e:
            logger.error(f"Error loading training data: {e}")
            return [], []
    
    def _generate_synthetic_data(self, count: int) -> Tuple[List[List[float]], List[int]]:
        """Generate synthetic training data."""
        logger.info(f"Generating {count} synthetic training samples")
        
        data = []
        labels = []
        
        for i in range(count):
            # Generate random features
            is_malware = np.random.choice([0, 1])
            
            if is_malware:
                # Malware-like features
                features = [
                    np.random.lognormal(10, 2),  # file_size (larger, more variable)
                    np.random.uniform(6.5, 8.0),  # entropy (higher)
                    np.random.uniform(0.0, 0.1),  # null_byte_ratio (lower)
                    np.random.uniform(0.3, 0.8),  # printable_ratio (variable)
                    np.random.poisson(10),  # suspicious_strings_count (higher)
                    np.random.poisson(5),   # crypto_strings_count (higher)
                    np.random.poisson(3),   # network_strings_count (higher)
                    np.random.uniform(100, 200),  # mean_byte_value
                    np.random.uniform(50, 100),   # std_byte_value
                    np.random.poisson(5),   # high_entropy_regions (higher)
                    np.random.poisson(20),  # longest_repeating_sequence
                    np.random.uniform(8, 20),  # name_length (variable)
                    np.random.uniform(0, 30),  # file_age_days
                    np.random.poisson(3),   # path_depth
                    np.random.poisson(2),   # url_count (higher)
                    np.random.poisson(1)    # ip_count (higher)
                ]
            else:
                # Benign file features
                features = [
                    np.random.lognormal(8, 1),   # file_size (smaller, less variable)
                    np.random.uniform(3.0, 6.5), # entropy (lower)
                    np.random.uniform(0.1, 0.3), # null_byte_ratio (higher)
                    np.random.uniform(0.6, 1.0), # printable_ratio (higher)
                    np.random.poisson(2),   # suspicious_strings_count (lower)
                    np.random.poisson(1),   # crypto_strings_count (lower)
                    np.random.poisson(1),   # network_strings_count (lower)
                    np.random.uniform(80, 150),   # mean_byte_value
                    np.random.uniform(30, 70),    # std_byte_value
                    np.random.poisson(1),   # high_entropy_regions (lower)
                    np.random.poisson(5),   # longest_repeating_sequence
                    np.random.uniform(5, 15),  # name_length
                    np.random.uniform(1, 365), # file_age_days (older)
                    np.random.poisson(2),   # path_depth
                    np.random.poisson(0),   # url_count (lower)
                    np.random.poisson(0)    # ip_count (lower)
                ]
            
            data.append(features)
            labels.append(is_malware)
        
        return data, labels
    
    def _save_models(self):
        """Save trained models to disk."""
        try:
            # Save models
            for name, model in self.models.items():
                if model is not None:
                    model_file = self.model_path / f"{name}_model.joblib"
                    joblib.dump(model, model_file)
            
            # Save scalers
            for name, scaler in self.scalers.items():
                if scaler is not None:
                    scaler_file = self.model_path / f"{name}_scaler.joblib"
                    joblib.dump(scaler, scaler_file)
            
            # Save anomaly detector
            if self.anomaly_detector is not None:
                anomaly_file = self.model_path / "anomaly_detector.joblib"
                joblib.dump(self.anomaly_detector, anomaly_file)
            
            # Save feature names
            feature_names_file = self.model_path / "feature_names.json"
            with open(feature_names_file, 'w') as f:
                json.dump(self.feature_names, f)
            
            logger.info("Models saved successfully")
            
        except Exception as e:
            logger.error(f"Error saving models: {e}")
    
    def _load_models(self):
        """Load trained models from disk."""
        try:
            # Load feature names
            feature_names_file = self.model_path / "feature_names.json"
            if feature_names_file.exists():
                with open(feature_names_file, 'r') as f:
                    self.feature_names = json.load(f)
            
            # Load models
            for model_name in self.model_configs.keys():
                model_file = self.model_path / f"{model_name}_model.joblib"
                scaler_file = self.model_path / f"{model_name}_scaler.joblib"
                
                if model_file.exists() and scaler_file.exists():
                    self.models[model_name] = joblib.load(model_file)
                    self.scalers[model_name] = joblib.load(scaler_file)
                    logger.info(f"Loaded {model_name} model")
            
            # Load anomaly detector
            anomaly_file = self.model_path / "anomaly_detector.joblib"
            if anomaly_file.exists():
                self.anomaly_detector = joblib.load(anomaly_file)
                logger.info("Loaded anomaly detector")
            
        except Exception as e:
            logger.warning(f"Error loading models: {e}")
    
    def get_model_info(self) -> Dict[str, Any]:
        """Get information about loaded models."""
        info = {
            'models_loaded': list(self.models.keys()),
            'scalers_loaded': list(self.scalers.keys()),
            'anomaly_detector_loaded': self.anomaly_detector is not None,
            'feature_count': len(self.feature_names),
            'feature_names': self.feature_names
        }
        
        # Model-specific info
        for name, model in self.models.items():
            if model is not None:
                info[f'{name}_info'] = {
                    'type': type(model).__name__,
                    'trained': hasattr(model, 'classes_')
                }
        
        return info